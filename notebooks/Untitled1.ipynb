{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import config as C\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import parse_camera_intrinsic as parse_camera_intrinsic\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from torchvision import transforms as T\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(C.TRAIN_CROPS_JSON, \"r\") as f:\n",
    "    train_gt = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "class PKURegressionDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, orientation_field, images_dir, whole_images_dir, max_size, max_whole_size, transforms=None):\n",
    "        super().__init__()\n",
    "        self.orientation_field = orientation_field\n",
    "        self.images_dir = images_dir\n",
    "        self.whole_images_dir = whole_images_dir\n",
    "        self.max_size = max_size\n",
    "        self.max_whole_size = max_whole_size\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        with open(C.TRAIN_CROPS_JSON, \"r\") as f:\n",
    "            self.gt = json.load(f)\n",
    "        \n",
    "        annotations =[]\n",
    "        for i in range(len(self.gt['annotations'])):\n",
    "            wx, wy, wz = self.gt['annotations'][i]['position']\n",
    "            if ((-50<wx<50) and (0<wy<50) and (0<wz<200) and (np.sqrt(wx**2 + wy**2 +wz**2) < 100)):\n",
    "                annotations.append(self.gt['annotations'][i])\n",
    "        self.gt['annotations'] = annotations\n",
    "    \n",
    "        cat_ids = set(ann['category_id'] for ann in self.gt['annotations'])\n",
    "        categories = [cat for cat in self.gt['categories'] if cat['id'] in cat_ids]\n",
    "        self.category_id_to_label = {\n",
    "            cat[\"id\"]: label\n",
    "            for label, cat in enumerate(sorted(categories, key=lambda x: x[\"id\"]))\n",
    "        }\n",
    "        self.images_jpeg = self.load_images()\n",
    "        self.whole_images_jpeg, self.ann_id_to_whole_image_filename = self.load_whole_images()\n",
    "        \n",
    "        self.p = parse_camera_intrinsic()\n",
    "        for k in self.p:\n",
    "            self.p[k] = float(self.p[k])\n",
    "    \n",
    "    def load_images(self):\n",
    "        images = {}\n",
    "        for image in tqdm(self.gt['images']):\n",
    "            path = os.path.join(self.images_dir, image['file_name'])\n",
    "            data = open(path, 'rb').read()\n",
    "            images[image['id']] = io.BytesIO(data)\n",
    "        return images\n",
    "    \n",
    "    def load_whole_images(self):\n",
    "        ann_id_to_whole_image_filename = {}\n",
    "        filenames = []\n",
    "        for image in self.gt['images']:\n",
    "            name, ext = image['file_name'].split('.')\n",
    "            ID, whole_image_name, ann_id = name.split(\"_\")\n",
    "            whole_image_name = ID + \"_\" + whole_image_name\n",
    "            filename = whole_image_name + '.' + ext\n",
    "            ann_id_to_whole_image_filename[int(ann_id)] = filename\n",
    "            filenames.append(filename)\n",
    "        filenames = list(set(filenames))\n",
    "        whole_images_jpeg = {}\n",
    "        for filename in tqdm(filenames):\n",
    "            path = os.path.join(self.whole_images_dir, filename)\n",
    "            data = open(path, 'rb').read()\n",
    "            whole_images_jpeg[filename] = io.BytesIO(data)\n",
    "        return whole_images_jpeg, ann_id_to_whole_image_filename \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.gt[\"annotations\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.load_image(idx)\n",
    "        whole_image,ann_id_to_whole_image_filename = self.load_whole_image(idx)\n",
    "        label = self.get_label(idx)\n",
    "        bbox_x, bbox_y, bbox_w, bbox_h = self.get_bbox(idx)\n",
    "        bbox_center_x, bbox_center_y = bbox_x + bbox_w / 2, bbox_y + bbox_h / 2\n",
    "        wx, wy, wz = self.get_position(idx)\n",
    "        orientation = self.get_orientation(idx)\n",
    "        filename = ann_id_to_whole_image_filename[idx]\n",
    "        \n",
    "        result =  dict(\n",
    "            image=image,\n",
    "            whole_image=whole_image,\n",
    "            label=label,\n",
    "            bbox=np.array([(bbox_center_x-self.p['cx'])/self.p['fx'], (bbox_center_y-self.p['cy'])/self.p['fy'], bbox_w/self.p['fx'], bbox_h/self.p['fy']]),\n",
    "            position=np.array([wx, wy, wz]),\n",
    "            distance=np.sqrt(wx**2 + wy**2 + wz**2),\n",
    "            orientation=np.array(orientation),\n",
    "            filename=filename\n",
    "            )\n",
    "        if self.transforms is not None:\n",
    "            result['image'] = self.transforms(result['image'])\n",
    "            result['whole_image'] = self.transforms(result['whole_image'])\n",
    "        return result\n",
    "    \n",
    "    def load_image(self, idx):\n",
    "        image_id = self.gt[\"annotations\"][idx][\"image_id\"]\n",
    "        image = self.decode_image(self.images_jpeg[image_id])\n",
    "        w, h = image.size\n",
    "        scale = self.max_size / max(w, h)\n",
    "        w_new, h_new = int(w * scale), int(h * scale)\n",
    "        image = image.resize((w_new, h_new), Image.LANCZOS)\n",
    "        new_image = Image.new(\"RGB\", (self.max_size, self.max_size))\n",
    "        new_image.paste(image, ((self.max_size - w_new) // 2, (self.max_size - h_new) // 2))\n",
    "        return new_image\n",
    "\n",
    "    def load_whole_image(self, idx):\n",
    "        ann_id = self.gt[\"annotations\"][idx][\"id\"]\n",
    "        image = self.decode_image(self.whole_images_jpeg[self.ann_id_to_whole_image_filename[ann_id]])\n",
    "        w, h = image.size\n",
    "        scale = self.max_whole_size / max(w, h)\n",
    "        w_new, h_new = int(w * scale), int(h * scale)\n",
    "        image = image.resize((w_new, h_new), Image.LANCZOS)\n",
    "        new_image = Image.new(\"RGB\", (self.max_whole_size, self.max_whole_size))\n",
    "        new_image.paste(image, ((self.max_whole_size - w_new) // 2, (self.max_whole_size - h_new) // 2))\n",
    "        return new_image\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_image(bytes_io):\n",
    "        image = Image.open(bytes_io)\n",
    "        image.load()\n",
    "        return image\n",
    "        \n",
    "    def get_label(self, idx):\n",
    "        return self.category_id_to_label[self.gt[\"annotations\"][idx][\"category_id\"]]\n",
    "    \n",
    "    def get_bbox(self, idx):\n",
    "        return self.gt[\"annotations\"][idx][\"bbox\"]\n",
    "    \n",
    "    def get_position(self, idx):\n",
    "        return self.gt[\"annotations\"][idx][\"position\"]\n",
    "    \n",
    "    def get_orientation(self, idx):\n",
    "        euler_angles = -1*np.array(self.gt[\"annotations\"][idx][self.orientation_field])\n",
    "        prom = euler_angles[0]*1\n",
    "        euler_angles[0] = euler_angles[1]*1\n",
    "        euler_angles[1] = prom*1\n",
    "        rotation = R.from_euler(\"YXZ\", euler_angles)\n",
    "        q = rotation.as_quat()\n",
    "        e = np.array([0,0,1])\n",
    "        q[:3] = q[:3] * np.dot(e, q[:3]) / (np.abs(np.dot(q[:3], e)))\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49115/49115 [00:01<00:00, 48590.54it/s]\n",
      "100%|██████████| 4219/4219 [00:02<00:00, 1770.69it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = PKURegressionDataset(\"orientation_relative\", C.TRAIN_CROPS_CALIBRATED, C.TRAIN_IMAGES, 256, 512, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.big_backbone = models.resnet101(pretrained=True)\n",
    "    \n",
    "    def extract_nl_features(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self,whole_image):\n",
    "        \n",
    "        nl_features = self.extract_nl_features(whole_image)\n",
    "        \n",
    "        return dict(nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1383 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ivb/.conda/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ivb/.conda/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ivb/.conda/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-16-3fa9103233aa>\", line 67, in __getitem__\n    whole_image,ann_id_to_whole_image_filename = self.load_whole_image(idx)\nTypeError: cannot unpack non-iterable Image object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f51145023ea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'label'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'filename'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/open-mmlab/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ivb/.conda/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ivb/.conda/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ivb/.conda/envs/open-mmlab/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-16-3fa9103233aa>\", line 67, in __getitem__\n    whole_image,ann_id_to_whole_image_filename = self.load_whole_image(idx)\nTypeError: cannot unpack non-iterable Image object\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(dl))\n",
    "for batch in dl:\n",
    "    for k in batch:\n",
    "        if k == 'label' or k == 'filename':\n",
    "            batch[k] = batch[k].long()\n",
    "        else:\n",
    "            batch[k] = batch[k].float()\n",
    "        batch[k] = batch[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['whole_image'])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
